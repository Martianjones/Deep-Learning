# Deep-Learning

编译平台VS Code，使用语言python 3.9

记录本人学习深度学习的过程，参考书目可在zh.d2l.ai获取

## 1. 引言

### 1.1 机器学习中的关键组件

1、可以用来学习的数据(data);

2、如何转换数据的模型(model);

3、一个目标函数(objective function),用来量化模型的有效性;

4、调整模型参数以优化目标函数的算法。

### 1.2 机器学习中的 各种问题

#### 1.2.1 监督学习

监督学习擅长在“给定输入特征”的情况下预测标签。每个“特征-标签”对都称为一个样本。

监督学习的学习过程一般可以分为三大步骤：

1、从已知大量数据样本中随机选取一个子集，为每个样本获取真实标签。有时，这些样本已有标签；有时这些样本可能需要被人工标记。这些输入和相应的标签一起构成了训练数据集；

2、选择有监督的学习算法，它将训练数据作为输入，并输出一个“已完成学习的模型”；

3、将之前没有见过样本特征放到这个“已完成学习的模型”中，使用模型的输出作为相应的预测。

回归 （regression）是最简单的监督学习任务之一。

#### 1.2.2 无监督学习

这类数据中不含有“目标”的机器学习问题通常被为 *无监督学习* （unsupervised learning）

无监督学习可以用于回答下列问题：

* *聚类* （clustering）问题：没有标签的情况下，我们是否能给数据分类呢？比如，给定一组照片，我们能把它们分成风景照片、狗、婴儿、猫和山峰的照片吗？同样，给定一组用户的网页浏览记录，我们能否将具有相似行为的用户聚类呢？
* *主成分分析* （principal component analysis）问题：我们能否找到少量的参数来准确地捕捉数据的线性相关属性？比如，一个球的运动轨迹可以用球的速度、直径和质量来描述。再比如，裁缝们已经开发出了一小部分参数，这些参数相当准确地描述了人体的形状，以适应衣服的需要。另一个例子：在欧几里得空间中是否存在一种（任意结构的）对象的表示，使其符号属性能够很好地匹配?这可以用来描述实体及其关系，例如“罗马”  “意大利”  “法国”  “巴黎”。
* *因果关系* （causality）和 *概率图模型* （probabilistic graphical models）问题：我们能否描述观察到的许多数据的根本原因？例如，如果我们有关于房价、污染、犯罪、地理位置、教育和工资的人口统计数据，我们能否简单地根据经验数据发现它们之间的关系？
* *生成对抗性网络* （generative adversarial networks）：为我们提供一种合成数据的方法，甚至像图像和音频这样复杂的非结构化数据。潜在的统计机制是检查真实和虚假数据是否相同的测试，它是无监督学习的另一个重要而令人兴奋的领域。

#### 1.2.3 强化学习

在强化学习问题中，智能体（agent）在一系列的时间步骤上与环境交互。 在每个特定时间点，智能体从环境接收一些 *观察* （observation），并且必须选择一个 *动作* （action），然后通过某种机制（有时称为执行器）将其传输回环境，最后智能体从环境中获得 *奖励* （reward）。

## 2. 预备知识

### 2.1 数据操作

#### 2.1.1 入门

张量表示一个由数值组成的数组，这个数组可能有多个维度。 具有一个轴的张量对应数学上的 *向量* （vector）； 具有两个轴的张量对应数学上的 *矩阵* （matrix）； 具有两个轴以上的张量没有特殊的数学名称。张量是矩阵向更高维度的推广。

##### 2.1.1.1 张量基本属性

张量具有以下几个关键属性：

* **阶（Rank）** ：张量的阶是指它的维度数量。例如，一个标量（单个数字）是0阶张量，一个向量（数字数组）是1阶张量，而一个矩阵（数字的二维数组）是2阶张量。
* **形状（Shape）** ：张量的形状描述了它在每个维度上的大小。例如，一个形状为(3, 5)的矩阵表示它有3行5列。
* **数据类型（Type）** ：张量中元素的数据类型，如float32、float64等。

##### 2.1.1.2 张量的几何解释

从几何角度来看，张量中的元素可以被视为某个高维空间中点的坐标。例如，一个2阶张量可以表示一个平面上的点集，而3阶张量可以表示一个立体空间中的点集。

##### 2.1.1.3 张量的运算

深度学习模型的训练和预测涉及大量的张量运算，包括但不限于：

* **逐元素运算** ：如加法、减法、乘法等，这些运算是对张量中每个元素独立进行的。
* **广播** ：当对形状不同的张量进行运算时，较小的张量会被“广播”以匹配较大张量的形状。
* **点积** ：也称为内积或张量积，是将两个张量的对应元素相乘后求和的运算。
* **变形** ：改变张量的形状而不改变其数据。
* **转置** ：在矩阵运算中，转置是将矩阵的行列互换的操作。

#### 2.1.2 运算符

对于任意具有相同形状的张量， 常见的标准算术运算符 `+`、-、`*`、`/`和 `**`都可以被升级为按元素运算。(**是求幂符号)

#### 2.1.3 广播机制

在上面的部分中，我们看到了如何在相同形状的两个张量上执行按元素操作。 在某些情况下，即使形状不同，我们仍然可以通过调用  *广播机制* （broadcasting mechanism）来执行按元素操作。 这种机制的工作方式如下：

* 通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状；
* 对生成的数组执行按元素操作。

#### 2.1.4 索引和切片

张量中的元素可以通过索引访问。与任何python数组一样：第一个元素的索引是0，最后一个元素是-1；可以指定范围以包含第一个元素和最后一个之前的元素。

除读取以外，我们还可以通过指定索引来将元素写入矩阵

#### 2.1.5 节省内存

运行一些操作可能会导致为新结果分配内存。 一个变量在经过重新赋值之后，会为其分配新的内存空间。

这可能是不可取的，原因有两个：

1. 首先，我们不想总是不必要地分配内存。在机器学习中，我们可能有数百兆的参数，并且在一秒内多次更新所有参数。通常情况下，我们希望原地执行这些更新；
2. 如果我们不原地更新，其他引用仍然会指向旧的内存位置，这样我们的某些代码可能会无意中引用旧的参数。

我们可以使用切片表示法，将操作的结果分配给先前分配的数组，例如Y[:]=`<expression>`

#### 2.1.6 转换为其他python对象

可以使用numpy()方法转换torch的张量，同时可以使用torch.tensor()方法来进行修改

要将大小为1的张量转换为python标量，可以调用item函数或python的内置函数

### 2.2 数据预处理

#### 2.2.1 读取数据集

使用pandas包来读取

#### 2.2.4 处理缺失值

注意，“NaN”项代表缺失值。 为了处理缺失的数据，典型的方法包括*插值法*和 *删除法* ， 其中插值法用一个替代值弥补缺失值，而删除法则直接忽略缺失值。

通过位置索引 `iloc`，我们将 `data`分成 `inputs`和 `outputs`， 其中前者为 `data`的前两列，而后者为 `data`的最后一列。 对于 `inputs`中缺少的数值，我们用同一列的均值替换“NaN”项。

对于 `inputs`中的类别值或离散值，我们将“NaN”视为一个类别。 由于“巷子类型”（“Alley”）列只接受两种类型的类别值“Pave”和“NaN”， `pandas`可以自动将此列转换为两列“Alley_Pave”和“Alley_nan”。 巷子类型为“Pave”的行会将“Alley_Pave”的值设置为1，“Alley_nan”的值设置为0。 缺少巷子类型的行会将“Alley_Pave”和“Alley_nan”分别设置为0和1。

### 2.3 线性代数

#### 2.3.1 标量

严格来说，仅包含一个数值被称为 *标量* （scalar）。

本书采用了数学表示法，其中标量变量由普通小写字母表示（例如，x、y和 z）。 本书用R表示所有（连续）*实数*标量的空间，之后将严格定义 *空间* （space）是什么， 但现在只要记住表达式x∈R是表示x是一个实值标量的正式形式。 符号∈称为“属于”，它表示“是集合中的成员”。

标量由只有一个元素的张量表示。

#### 2.3.2 向量

向量可以被视为标量值组成的列表。 这些标量值被称为向量的 *元素* （element）或 *分量* （component）。当向量表示数据集中的样本时，它们的值具有一定的现实意义。人们通过一维张量表示向量。我们可以使用下标来引用向量的任一元素。

向量只是一个数字数组，就像每个数组都有一个长度一样，每个向量也是如此。向量的长度通常称为向量的 *维度* （dimension）。

当用张量表示一个向量（只有一个轴）时，我们也可以通过 `<span class="pre">.shape</span>`属性访问向量的长度。 形状（shape）是一个元素组，列出了张量沿每个轴的长度（维数）。 对于只有一个轴的张量，形状只有一个元素。

为了清楚起见，我们在此明确一下： *向量*或*轴*的维度被用来表示*向量*或*轴*的长度，即向量或轴的元素数量。 然而，张量的维度用来表示张量具有的轴数。 在这个意义上，张量的某个轴的维数就是这个轴的长度。

#### 2.3.3 矩阵

正如向量将标量从零阶推广到一阶，矩阵将向量从一阶推广到二阶。

矩阵是有用的数据结构：它们允许我们组织具有不同模式的数据。尽管单个向量的默认方向是列向量，但在表示表格数据集的矩阵中， 将每个数据样本作为矩阵中的行向量更为常见。

#### 2.3.4 张量

就像向量是标量的推广，矩阵是向量的推广一样，我们可以构建具有更多轴的数据结构。 张量（本小节中的“张量”指代数对象）是描述具有任意数量轴的维数组的通用方法。

#### 2.3.5 张量算法的基本性质

标量、向量、矩阵和任意数量轴的张量（本小节中的“张量”指代数对象）有一些实用的属性。

具体而言，两个矩阵的按元素乘法称为 *Hadamard积* （Hadamard product）

将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘。

#### 2.3.6 降维

我们可以对任意张量进行的一个有用的操作是计算其元素的和。

我们可以使用sum函数来表示任意形状张量的元素和。

默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量。 我们还可以指定张量沿哪一个轴来通过求和降低维度。

##### 2.3.6.1 非降维求和

但是，有时在调用函数来计算总和或均值时保持轴数不变会很有用。例如，由于 `sum_A`在对每行进行求和后仍保持两个轴，我们可以通过广播将 `A`除以 `sum_A`。cumsum函数不会沿任何轴降低输入张量的维度，该函数会进行逐行求和。

#### 2.3.7 点积

我们已经学习了按元素操作、求和及平均值。 另一个最基本的操作之一是点积。给定两个向量x,y，他们的点积是相同位置按元素乘积的和

点积在很多场合都很有用。将两个向量规范化得到单位长度后，点积表示它们夹角的余弦。

#### 2.3.8 矩阵-向量积

矩阵向量积Ax是一个长度为m的列向量，其第i个元素是点积aiTx；

在代码中使用张量表示矩阵-向量积，我们使用与点积相同的 `dot`函数。当我们为矩阵A和向量x调用dot时，会执行矩阵-向量积。需要注意的时矩阵的列数必须与x的维数相同。

#### 2.3.9 矩阵-矩阵乘法

在掌握点积和矩阵-向量积的知识后， 那么 **矩阵-矩阵乘法** （matrix-matrix multiplication）应该很简单。可以想象是由多个列向量与矩阵相乘得到一个新的矩阵。

#### 2.3.10 范数

线性代数中最有用的一些运算符是 *范数* （norm）。 非正式地说，向量的*范数*是表示一个向量有多大。 即范数是向量长度或大小的一种度量方式。这里考虑的 *大小* （size）概念不涉及维度，而是分量的大小。在线性代数中，向量范数是将向量映射到标量的函数f。给定任意向量x，向量范数要满足一些属性。第一个性质是：如果我们按常熟银子α缩放向量的所有元素，其范数也会按相同常数因子的绝对值缩放；

第二个性质是熟悉的三角不等式：f(x+y)<=f(x)+f(y)

第三个性质是范数必须非负，范数为零当且仅当向量全由0组成

##### 2.3.10.1 范数和目标

在深度学习中，我们经常试图解决优化问题： *最大化*分配给观测数据的概率; *最小化*预测和真实观测之间的距离。 用向量表示物品（如单词、产品或新闻文章），以便最小化相似项目之间的距离，最大化不同项目之间的距离。 目标，或许是深度学习算法最重要的组成部分（除了数据），通常被表达为范数。

### 2.4 微积分

在2500年前，古希腊人把一个多边形分成三角形，并把它们的面积相加，才找到计算多边形面积的方法。 为了求出曲线形状（比如圆）的面积，古希腊人在这样的形状上刻内接多边形。内接多边形的等长边越多，就越接近圆。 这个过程也被称为 *逼近法* （method of exhaustion）。

事实上，逼近法就是 *积分* （integral calculus）的起源。

在深度学习中，我们“训练”模型，不断更新它们，使它们在看到越来越多的数据时变得越来越好。 通常情况下，变得更好意味着最小化一个 *损失函数* （loss function）， 即一个衡量“模型有多糟糕”这个问题的分数。 最终，我们真正关心的是生成一个模型，它能够在从未见过的数据上表现良好。 但“训练”模型只能将模型与我们实际能看到的数据相拟合。 因此，我们可以将拟合模型的任务分解为两个关键问题：

* *优化* （optimization）：用模型拟合观测数据的过程；
* *泛化* （generalization）：数学原理和实践者的智慧，能够指导我们生成出有效性超出用于训练的数据集本身的模型。

#### 2.4.1 导数和微分

我们首先讨论导数的计算，这是几乎所有深度学习优化算法的关键步骤。 在深度学习中，我们通常选择对于模型参数可微的损失函数。 简而言之，对于每个参数， 如果我们把这个参数*增加*或*减少*一个无穷小的量，可以知道损失会以多快的速度增加或减少。

#### 2.4.2 偏导数

到目前为止，我们只讨论了仅含一个变量的函数的微分。 在深度学习中，函数通常依赖于许多变量。 因此，我们需要将微分的思想推广到 *多元函数* （multivariate function）上。

#### 2.4.3 梯度

我们可以连结一个多元函数对其所有变量的偏导数，以得到该函数的 *梯度* （gradient）向量。

#### 2.4.4 链式法则

然而，上面方法可能很难找到梯度。 这是因为在深度学习中，多元函数通常是 *复合* （composite）的， 所以难以应用上述任何规则来微分这些函数。 幸运的是，链式法则可以被用来微分复合函数。

### 2.5 自动微分

正如 [2.4节](https://zh.d2l.ai/chapter_preliminaries/calculus.html#sec-calculus)中所说，求导是几乎所有深度学习优化算法的关键步骤。 虽然求导的计算很简单，只需要一些基本的微积分。 但对于复杂的模型，手工进行更新是一件很痛苦的事情（而且经常容易出错）。

深度学习框架通过自动计算导数，即 *自动微分* （automatic differentiation）来加快求导。 实际中，根据设计好的模型，系统会构建一个 *计算图* （computational graph）， 来跟踪计算是哪些数据通过哪些操作组合起来产生输出。 自动微分使系统能够随后反向传播梯度。 这里， *反向传播* （backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。

#### 2.5.1 非标量变量的反向传播

当 `y`不是标量时，向量 `y`关于向量 `x`的导数的最自然解释是一个矩阵。 对于高阶和高维的 `y`和 `x`，求导的结果可以是一个高阶张量。

然而，虽然这些更奇特的对象确实出现在高级机器学习中（包括深度学习中）， 但当调用向量的反向计算时，我们通常会试图计算一批训练样本中每个组成部分的损失函数的导数。 这里，我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和。

#### 2.5.2 分离计算

有时，我们希望将某些计算移动到记录的计算图之外。 例如，假设 `y`是作为 `x`的函数计算的，而 `z`则是作为 `y`和 `x`的函数计算的。 想象一下，我们想计算 `z`关于 `x`的梯度，但由于某种原因，希望将 `y`视为一个常数， 并且只考虑到 `x`在 `y`被计算后发挥的作用。

这里可以分离 `y`来返回一个新变量 `u`，该变量与 `y`具有相同的值， 但丢弃计算图中如何计算 `y`的任何信息。 换句话说，梯度不会向后流经 `u`到 `x`。 因此，下面的反向传播函数计算 `z=u*x`关于 `x`的偏导数，同时将 `u`作为常数处理， 而不是 `z=x*x*x`关于 `x`的偏导数。

#### 2.5.3 Python控制流的梯度计算

使用自动微分的一个好处是： 即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度。 在下面的代码中，`while`循环的迭代次数和 `if`语句的结果都取决于输入 `a`的值。

### 2.6 概率

简单地说，机器学习就是根据概率做出预测。

#### 2.6.1 基本概率论

在统计学中，我们把从概率分布中抽取样本的过程称为 *抽样* （sampling）。 笼统来说，可以把 *分布* （distribution）看作对事件的概率分配。将概率分配给一些离散选择的分布称为 *多项分布* （multinomial distribution）。

为了抽取一个样本，即掷骰子，我们只需传入一个概率向量。 输出是另一个相同长度的向量：它在索引处的值是采样结果中出现的次数。在估计一个骰子的公平性时，我们希望从同一分布中生成多个样本。如果用Python的for循环来完成这个任务，速度会巨慢。因此我们使用深度学习框架的函数同时抽取多个样本，得到我们想要的任意形状的独立样本数组。

##### 2.6.1.1 概率论公理

在处理骰子掷出时，我们将集合S={1，2，3，4，5，6} 称为 *样本空间* （sample space）或 *结果空间* （outcome space）， 其中每个元素都是 *结果* （outcome）。  *事件* （event）是一组给定样本空间的随机结果。*概率* （probability）可以被认为是将集合映射到真实值的函数。

在给定的样本空间S中，事件A的概率， 表示为P(A)，满足以下属性：

* 对于任意事件，其概率从不会是负数
* 整个样本空间的概率为1
* 对于 *互斥* （mutually exclusive）事件（对于所有都有）的任意一个可数序列，序列中任意一个事件发生的概率等于它们各自发生的概率之和，即。

##### 2.6.1.2 随机变量

在我们掷骰子的随机实验中，我们引入了 *随机变量* （random variable）的概念。 随机变量几乎可以是任何数量，并且它可以在随机实验的一组可能性中取一个值。为了简化符号，一方面，我们可以将P(A)表示为随机变量X上的 *分布* （distribution）： 分布告诉我们获得某一值的概率。 另一方面，我们可以简单用P(a)表示随机变量取值a的概率。

请注意， *离散* （discrete）随机变量（如骰子的每一面） 和 *连续* （continuous）随机变量（如人的体重和身高）之间存在微妙的区别。

#### 2.6.2 处理多个随机变量

很多时候，我们会考虑多个随机变量。 比如，我们可能需要对疾病和症状之间的关系进行建模。 给定一个疾病和一个症状，比如“流感”和“咳嗽”，以某个概率存在或不存在于某个患者身上。 我们需要估计这些概率以及概率之间的关系，以便我们可以运用我们的推断来实现更好的医疗服务。

再举一个更复杂的例子：图像包含数百万像素，因此有数百万个随机变量。 在许多情况下，图像会附带一个 *标签* （label），标识图像中的对象。 我们也可以将标签视为一个随机变量。 我们甚至可以将所有元数据视为随机变量，例如位置、时间、光圈、焦距、ISO、对焦距离和相机类型。 所有这些都是联合发生的随机变量。 当我们处理多个随机变量时，会有若干个变量是我们感兴趣的。

##### 2.6.2.1 联合概率

第一个被称为 *联合概率* （joint probability）P(A=a,B=b)。 给定任意值a和b，联合概率可以回答：A=a和B=b同时满足的概率是多少？ 请注意，对于任何a和b的取值，P(A=a,B=b)<=P(A=a)。

##### 2.6.2.2 条件概率

P(B=b|A=a)表示：它是B=b的概率，前提是A=a已发生。

##### 2.6.2.3 贝叶斯定理

##### 2.6.2.4 边际化

为了能进行事件概率求和，我们需要 *求和法则* （sum rule）， 即的概率相当于计算的所有可能选择，并将所有选择的联合概率聚合在一起。这也称为 *边际化* （marginalization）。 边际化结果的概率或分布称为 *边际概率* （marginal probability） 或 *边际分布* （marginal distribution）。

##### 2.6.2.5 独立性

另一个有用属性是 *依赖* （dependence）与 *独立* （independence）。 如果两个随机变量A和B是独立的，意味着事件A的发生跟事件B的发生无关。 在这种情况下，统计学家通常将这一点表述为A⊥B。 根据贝叶斯定理，马上就能同样得到P(A|B)=P(A)。 在所有其他情况下，我们称A和B依赖。

#### 2.6.3 期望和方差

### 2.7 查阅文档

#### 2.7.1 查找模块中的所有函数和类

为了知道模块中可以调用哪些函数和类，可以调用dir函数。 通常可以忽略以"__"(双下划线)开始和结束的函数，它们时Python中的特殊对象，或以单个"_"(单下划线)开始的函数，他们通常是内部函数。根据剩余的函数名或属性名，我们可能会猜测这个模块提供了各种生成随机数的方法，包括从均匀分布、正态分布和多项分布中采样。

#### 2.7.2 查找特定函数和类的用法

有关如何使用给定函数或类的更具体说明，可以调用 `help`函数。

## 线性神经网络

### 3.1 线性回归
