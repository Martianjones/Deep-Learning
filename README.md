# Deep-Learning

## 1. 引言

### 1.1 机器学习中的关键组件

1、可以用来学习的数据(data);

2、如何转换数据的模型(model);

3、一个目标函数(objective function),用来量化模型的有效性;

4、调整模型参数以优化目标函数的算法。

### 1.2 机器学习中的 各种问题

#### 1.2.1 监督学习

监督学习擅长在“给定输入特征”的情况下预测标签。每个“特征-标签”对都称为一个样本。

监督学习的学习过程一般可以分为三大步骤：

1、从已知大量数据样本中随机选取一个子集，为每个样本获取真实标签。有时，这些样本已有标签；有时这些样本可能需要被人工标记。这些输入和相应的标签一起构成了训练数据集；

2、选择有监督的学习算法，它将训练数据作为输入，并输出一个“已完成学习的模型”；

3、将之前没有见过样本特征放到这个“已完成学习的模型”中，使用模型的输出作为相应的预测。

回归 （regression）是最简单的监督学习任务之一。

#### 1.2.2 无监督学习

这类数据中不含有“目标”的机器学习问题通常被为 *无监督学习* （unsupervised learning）

无监督学习可以用于回答下列问题：

* *聚类* （clustering）问题：没有标签的情况下，我们是否能给数据分类呢？比如，给定一组照片，我们能把它们分成风景照片、狗、婴儿、猫和山峰的照片吗？同样，给定一组用户的网页浏览记录，我们能否将具有相似行为的用户聚类呢？
* *主成分分析* （principal component analysis）问题：我们能否找到少量的参数来准确地捕捉数据的线性相关属性？比如，一个球的运动轨迹可以用球的速度、直径和质量来描述。再比如，裁缝们已经开发出了一小部分参数，这些参数相当准确地描述了人体的形状，以适应衣服的需要。另一个例子：在欧几里得空间中是否存在一种（任意结构的）对象的表示，使其符号属性能够很好地匹配?这可以用来描述实体及其关系，例如“罗马”  “意大利”  “法国”  “巴黎”。
* *因果关系* （causality）和 *概率图模型* （probabilistic graphical models）问题：我们能否描述观察到的许多数据的根本原因？例如，如果我们有关于房价、污染、犯罪、地理位置、教育和工资的人口统计数据，我们能否简单地根据经验数据发现它们之间的关系？
* *生成对抗性网络* （generative adversarial networks）：为我们提供一种合成数据的方法，甚至像图像和音频这样复杂的非结构化数据。潜在的统计机制是检查真实和虚假数据是否相同的测试，它是无监督学习的另一个重要而令人兴奋的领域。

#### 1.2.3 强化学习

在强化学习问题中，智能体（agent）在一系列的时间步骤上与环境交互。 在每个特定时间点，智能体从环境接收一些 *观察* （observation），并且必须选择一个 *动作* （action），然后通过某种机制（有时称为执行器）将其传输回环境，最后智能体从环境中获得 *奖励* （reward）。

## 2. 预备知识

### 2.1 数据操作

#### 2.1.1 入门

张量表示一个由数值组成的数组，这个数组可能有多个维度。 具有一个轴的张量对应数学上的 *向量* （vector）； 具有两个轴的张量对应数学上的 *矩阵* （matrix）； 具有两个轴以上的张量没有特殊的数学名称。张量是矩阵向更高维度的推广。

##### 2.1.1.1 张量基本属性

张量具有以下几个关键属性：

* **阶（Rank）** ：张量的阶是指它的维度数量。例如，一个标量（单个数字）是0阶张量，一个向量（数字数组）是1阶张量，而一个矩阵（数字的二维数组）是2阶张量。
* **形状（Shape）** ：张量的形状描述了它在每个维度上的大小。例如，一个形状为(3, 5)的矩阵表示它有3行5列。
* **数据类型（Type）** ：张量中元素的数据类型，如float32、float64等。

##### 2.1.1.2 张量的几何解释

从几何角度来看，张量中的元素可以被视为某个高维空间中点的坐标。例如，一个2阶张量可以表示一个平面上的点集，而3阶张量可以表示一个立体空间中的点集。

##### 2.1.1.3 张量的运算

深度学习模型的训练和预测涉及大量的张量运算，包括但不限于：

* **逐元素运算** ：如加法、减法、乘法等，这些运算是对张量中每个元素独立进行的。
* **广播** ：当对形状不同的张量进行运算时，较小的张量会被“广播”以匹配较大张量的形状。
* **点积** ：也称为内积或张量积，是将两个张量的对应元素相乘后求和的运算。
* **变形** ：改变张量的形状而不改变其数据。
* **转置** ：在矩阵运算中，转置是将矩阵的行列互换的操作。

#### 2.1.2 运算符

对于任意具有相同形状的张量， 常见的标准算术运算符 `+`、-、`*`、`/`和 `**`都可以被升级为按元素运算。(**是求幂符号)

#### 2.1.3 广播机制

在上面的部分中，我们看到了如何在相同形状的两个张量上执行按元素操作。 在某些情况下，即使形状不同，我们仍然可以通过调用  *广播机制* （broadcasting mechanism）来执行按元素操作。 这种机制的工作方式如下：

* 通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状；
* 对生成的数组执行按元素操作。

#### 2.1.4 索引和切片

张量中的元素可以通过索引访问。与任何python数组一样：第一个元素的索引是0，最后一个元素是-1；可以指定范围以包含第一个元素和最后一个之前的元素。

除读取以外，我们还可以通过指定索引来将元素写入矩阵

#### 2.1.5 节省内存

运行一些操作可能会导致为新结果分配内存。 一个变量在经过重新赋值之后，会为其分配新的内存空间。

这可能是不可取的，原因有两个：

1. 首先，我们不想总是不必要地分配内存。在机器学习中，我们可能有数百兆的参数，并且在一秒内多次更新所有参数。通常情况下，我们希望原地执行这些更新；
2. 如果我们不原地更新，其他引用仍然会指向旧的内存位置，这样我们的某些代码可能会无意中引用旧的参数。

我们可以使用切片表示法，将操作的结果分配给先前分配的数组，例如Y[:]=`<expression>`

#### 2.1.6 转换为其他python对象

可以使用numpy()方法转换torch的张量，同时可以使用torch.tensor()方法来进行修改

要将大小为1的张量转换为python标量，可以调用item函数或python的内置函数
