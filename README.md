# Deep-Learning

编译平台VS Code，使用语言python 3.9

记录本人学习深度学习的过程，参考书目可在zh.d2l.ai获取

## 1. 引言

### 1.1 机器学习中的关键组件

1、可以用来学习的数据(data);

2、如何转换数据的模型(model);

3、一个目标函数(objective function),用来量化模型的有效性;

4、调整模型参数以优化目标函数的算法。

### 1.2 机器学习中的 各种问题

#### 1.2.1 监督学习

监督学习擅长在“给定输入特征”的情况下预测标签。每个“特征-标签”对都称为一个样本。

监督学习的学习过程一般可以分为三大步骤：

1、从已知大量数据样本中随机选取一个子集，为每个样本获取真实标签。有时，这些样本已有标签；有时这些样本可能需要被人工标记。这些输入和相应的标签一起构成了训练数据集；

2、选择有监督的学习算法，它将训练数据作为输入，并输出一个“已完成学习的模型”；

3、将之前没有见过样本特征放到这个“已完成学习的模型”中，使用模型的输出作为相应的预测。

回归 （regression）是最简单的监督学习任务之一。

#### 1.2.2 无监督学习

这类数据中不含有“目标”的机器学习问题通常被为 *无监督学习* （unsupervised learning）

无监督学习可以用于回答下列问题：

* *聚类* （clustering）问题：没有标签的情况下，我们是否能给数据分类呢？比如，给定一组照片，我们能把它们分成风景照片、狗、婴儿、猫和山峰的照片吗？同样，给定一组用户的网页浏览记录，我们能否将具有相似行为的用户聚类呢？
* *主成分分析* （principal component analysis）问题：我们能否找到少量的参数来准确地捕捉数据的线性相关属性？比如，一个球的运动轨迹可以用球的速度、直径和质量来描述。再比如，裁缝们已经开发出了一小部分参数，这些参数相当准确地描述了人体的形状，以适应衣服的需要。另一个例子：在欧几里得空间中是否存在一种（任意结构的）对象的表示，使其符号属性能够很好地匹配?这可以用来描述实体及其关系，例如“罗马”  “意大利”  “法国”  “巴黎”。
* *因果关系* （causality）和 *概率图模型* （probabilistic graphical models）问题：我们能否描述观察到的许多数据的根本原因？例如，如果我们有关于房价、污染、犯罪、地理位置、教育和工资的人口统计数据，我们能否简单地根据经验数据发现它们之间的关系？
* *生成对抗性网络* （generative adversarial networks）：为我们提供一种合成数据的方法，甚至像图像和音频这样复杂的非结构化数据。潜在的统计机制是检查真实和虚假数据是否相同的测试，它是无监督学习的另一个重要而令人兴奋的领域。

#### 1.2.3 强化学习

在强化学习问题中，智能体（agent）在一系列的时间步骤上与环境交互。 在每个特定时间点，智能体从环境接收一些 *观察* （observation），并且必须选择一个 *动作* （action），然后通过某种机制（有时称为执行器）将其传输回环境，最后智能体从环境中获得 *奖励* （reward）。

## 2. 预备知识

### 2.1 数据操作

#### 2.1.1 入门

张量表示一个由数值组成的数组，这个数组可能有多个维度。 具有一个轴的张量对应数学上的 *向量* （vector）； 具有两个轴的张量对应数学上的 *矩阵* （matrix）； 具有两个轴以上的张量没有特殊的数学名称。张量是矩阵向更高维度的推广。

##### 2.1.1.1 张量基本属性

张量具有以下几个关键属性：

* **阶（Rank）** ：张量的阶是指它的维度数量。例如，一个标量（单个数字）是0阶张量，一个向量（数字数组）是1阶张量，而一个矩阵（数字的二维数组）是2阶张量。
* **形状（Shape）** ：张量的形状描述了它在每个维度上的大小。例如，一个形状为(3, 5)的矩阵表示它有3行5列。
* **数据类型（Type）** ：张量中元素的数据类型，如float32、float64等。

##### 2.1.1.2 张量的几何解释

从几何角度来看，张量中的元素可以被视为某个高维空间中点的坐标。例如，一个2阶张量可以表示一个平面上的点集，而3阶张量可以表示一个立体空间中的点集。

##### 2.1.1.3 张量的运算

深度学习模型的训练和预测涉及大量的张量运算，包括但不限于：

* **逐元素运算** ：如加法、减法、乘法等，这些运算是对张量中每个元素独立进行的。
* **广播** ：当对形状不同的张量进行运算时，较小的张量会被“广播”以匹配较大张量的形状。
* **点积** ：也称为内积或张量积，是将两个张量的对应元素相乘后求和的运算。
* **变形** ：改变张量的形状而不改变其数据。
* **转置** ：在矩阵运算中，转置是将矩阵的行列互换的操作。

#### 2.1.2 运算符

对于任意具有相同形状的张量， 常见的标准算术运算符 `+`、-、`*`、`/`和 `**`都可以被升级为按元素运算。(**是求幂符号)

#### 2.1.3 广播机制

在上面的部分中，我们看到了如何在相同形状的两个张量上执行按元素操作。 在某些情况下，即使形状不同，我们仍然可以通过调用  *广播机制* （broadcasting mechanism）来执行按元素操作。 这种机制的工作方式如下：

* 通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状；
* 对生成的数组执行按元素操作。

#### 2.1.4 索引和切片

张量中的元素可以通过索引访问。与任何python数组一样：第一个元素的索引是0，最后一个元素是-1；可以指定范围以包含第一个元素和最后一个之前的元素。

除读取以外，我们还可以通过指定索引来将元素写入矩阵

#### 2.1.5 节省内存

运行一些操作可能会导致为新结果分配内存。 一个变量在经过重新赋值之后，会为其分配新的内存空间。

这可能是不可取的，原因有两个：

1. 首先，我们不想总是不必要地分配内存。在机器学习中，我们可能有数百兆的参数，并且在一秒内多次更新所有参数。通常情况下，我们希望原地执行这些更新；
2. 如果我们不原地更新，其他引用仍然会指向旧的内存位置，这样我们的某些代码可能会无意中引用旧的参数。

我们可以使用切片表示法，将操作的结果分配给先前分配的数组，例如Y[:]=`<expression>`

#### 2.1.6 转换为其他python对象

可以使用numpy()方法转换torch的张量，同时可以使用torch.tensor()方法来进行修改

要将大小为1的张量转换为python标量，可以调用item函数或python的内置函数

### 2.2 数据预处理

#### 2.2.1 读取数据集

使用pandas包来读取

#### 2.2.4 处理缺失值

注意，“NaN”项代表缺失值。 为了处理缺失的数据，典型的方法包括*插值法*和 *删除法* ， 其中插值法用一个替代值弥补缺失值，而删除法则直接忽略缺失值。

通过位置索引 `iloc`，我们将 `data`分成 `inputs`和 `outputs`， 其中前者为 `data`的前两列，而后者为 `data`的最后一列。 对于 `inputs`中缺少的数值，我们用同一列的均值替换“NaN”项。

对于 `inputs`中的类别值或离散值，我们将“NaN”视为一个类别。 由于“巷子类型”（“Alley”）列只接受两种类型的类别值“Pave”和“NaN”， `pandas`可以自动将此列转换为两列“Alley_Pave”和“Alley_nan”。 巷子类型为“Pave”的行会将“Alley_Pave”的值设置为1，“Alley_nan”的值设置为0。 缺少巷子类型的行会将“Alley_Pave”和“Alley_nan”分别设置为0和1。

### 2.3 线性代数

#### 2.3.1 标量

严格来说，仅包含一个数值被称为 *标量* （scalar）。

本书采用了数学表示法，其中标量变量由普通小写字母表示（例如，x、y和 z）。 本书用R表示所有（连续）*实数*标量的空间，之后将严格定义 *空间* （space）是什么， 但现在只要记住表达式x∈R是表示x是一个实值标量的正式形式。 符号∈称为“属于”，它表示“是集合中的成员”。

标量由只有一个元素的张量表示。

#### 2.3.2 向量

向量可以被视为标量值组成的列表。 这些标量值被称为向量的 *元素* （element）或 *分量* （component）。当向量表示数据集中的样本时，它们的值具有一定的现实意义。人们通过一维张量表示向量。我们可以使用下标来引用向量的任一元素。

向量只是一个数字数组，就像每个数组都有一个长度一样，每个向量也是如此。向量的长度通常称为向量的 *维度* （dimension）。

当用张量表示一个向量（只有一个轴）时，我们也可以通过 `<span class="pre">.shape</span>`属性访问向量的长度。 形状（shape）是一个元素组，列出了张量沿每个轴的长度（维数）。 对于只有一个轴的张量，形状只有一个元素。

为了清楚起见，我们在此明确一下： *向量*或*轴*的维度被用来表示*向量*或*轴*的长度，即向量或轴的元素数量。 然而，张量的维度用来表示张量具有的轴数。 在这个意义上，张量的某个轴的维数就是这个轴的长度。

#### 2.3.3 矩阵

正如向量将标量从零阶推广到一阶，矩阵将向量从一阶推广到二阶。

矩阵是有用的数据结构：它们允许我们组织具有不同模式的数据。尽管单个向量的默认方向是列向量，但在表示表格数据集的矩阵中， 将每个数据样本作为矩阵中的行向量更为常见。

#### 2.3.4 张量

就像向量是标量的推广，矩阵是向量的推广一样，我们可以构建具有更多轴的数据结构。 张量（本小节中的“张量”指代数对象）是描述具有任意数量轴的维数组的通用方法。

#### 2.3.5 张量算法的基本性质

标量、向量、矩阵和任意数量轴的张量（本小节中的“张量”指代数对象）有一些实用的属性。

具体而言，两个矩阵的按元素乘法称为 *Hadamard积* （Hadamard product）

将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘。

#### 2.3.6 降维

我们可以对任意张量进行的一个有用的操作是计算其元素的和。

我们可以使用sum函数来表示任意形状张量的元素和。

默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量。 我们还可以指定张量沿哪一个轴来通过求和降低维度。

##### 2.3.6.1 非降维求和

但是，有时在调用函数来计算总和或均值时保持轴数不变会很有用。例如，由于 `sum_A`在对每行进行求和后仍保持两个轴，我们可以通过广播将 `A`除以 `sum_A`。cumsum函数不会沿任何轴降低输入张量的维度，该函数会进行逐行求和。

#### 2.3.7 点积

我们已经学习了按元素操作、求和及平均值。 另一个最基本的操作之一是点积。给定两个向量x,y，他们的点积是相同位置按元素乘积的和

点积在很多场合都很有用。将两个向量规范化得到单位长度后，点积表示它们夹角的余弦。
